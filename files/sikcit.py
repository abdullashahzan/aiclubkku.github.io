# -*- coding: utf-8 -*-
"""Sikcit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o3Si8QYxvkARs3p7bVflDlTwDzTVNq6f

#**Installing Sickit-learn library**#
"""

!pip install sklearn

import sklearn
sklearn.__version__

import numpy as np
import matplotlib.pyplot as plt

"""#**Simple Linear Regression**#

# **Training data**
"""

# X represents the features of our training data, the diameters of the pizzas. (Note: Uppercase letters indicate matrix and lowercase indicate vectors in sickit-learn convention)
X = np.array([
    [6],
    [8],
    [10],
    [14],
    [18]
]).reshape(-1,1)
# -1 means unkown rows (i.e. computer can decide) and 1 means we want only 1 column
print(X)

# y is a vector representing prize of the pizza 
y = [7, 9, 13, 17.5, 18]

"""# **Plotting a graph for given data**"""

plt.figure()
plt.plot(X,y, ".")
plt.axis([0,25,0,25])
plt.grid(True)
plt.show()

"""# **Importing model and fitting data**"""

from sklearn.linear_model import LinearRegression

# Creating an instance (An instance is an object that belongs to a particular class and in this case the object is LinearRegression() from the class LinearRegression)
model = LinearRegression()

# Fitting the data into the model

model.fit(X,y)

"""# **Predicting the price of a pizza with a diameter that has never been seen before**"""

# tp stands for test pizza

tp = np.array([
    [12]
]).reshape(-1,1)

# Predicting price through the model that we created.
price = model.predict(tp)

# .2f stands for 2 decimal places only
print("$%.2f" % price)

"""# **Evaluating our model**"""

"""
To Evaluate our model we need to have a data which is correct and without any errors. Then, we will compare the data that computer generates with the correct data to check how much the error in our model is.
"""

# X0 stands for accurate matrix which contains the diameter of our pizza and y0 stands for accurate vector which contain the price of the pizza
X0 = np.array([
    [6],
    [8],
    [10],
    [14],
    [18]
]).reshape(-1,1)

y0 = [12, 16, 20, 28, 36]


# X1 stands for training matrix and y1 stands for training vector

X1 = np.array([
    [16],
    [18],
    [20],
    [24],
    [25]
]).reshape(-1,1)

y1 = [33, 38, 44, 47, 51]


"""
Now we fit our data that is not accurate in the model and on the other hand we use the accurate data to test how good our model is
"""

model.fit(X1, y1)

# score stands for how good our model predicted the result in other terms how good is the accuracy of our model

score = model.score(X0, y0)

print("%.2f" %score)

"""# **End of linear regression algorithm**

# **K Nearest Neighbour**

# **Training data**
"""

X_train = np.array([
    [158, 64],
    [170, 86],
    [183,84],
    [191, 80],
    [155, 49],
    [163, 59],
    [180, 67],
    [158, 54],
    [178, 77]
])

y_train = ['male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'female']

"""# **Plotting data**"""

plt.figure()

for i, x in enumerate(X_train):
  #x[0] is our height and x[1] is our weight. We have taken them as our X and Y values for the graph
  plt.scatter(x[0], x[1], c='k', marker='x' if y_train[i] == 'male' else 'D')

plt.show()

"""# **Importing modules**"""

from sklearn.preprocessing import LabelBinarizer
from sklearn.neighbors import KNeighborsClassifier

"""# **Initialization**"""

#When the data is only in 2 parts for eg in this case male and female, we can use built in preprocessing function from sklearn library called label binarizer. It converts one value to 0 and the other value to 1.
#We use this function because it can make our data from string to number and at any time back to string which will be usefull for us to convert output data (which will also be in numbers) 
#from our model back to human readable string.
lb = LabelBinarizer()
#we do .reshape(-1) and not .reshape(-1, 1) because our vector y only has one row and we convert that row to column
y_train_binarized = lb.fit_transform(y_train).reshape(-1)
#K is a variable which defines the number of neighbours we will check our data with
K = 3

"""# **Fitting data and training model**"""

model = KNeighborsClassifier(n_neighbors=K)

model.fit(X_train, y_train_binarized)

"""# **Predicting output**"""

prediction_binarized = model.predict(np.array([154,70]).reshape(1, -1))
#the label binarizer function is useful here because it converts our 0's and 1's back to male, female
predicted_label = lb.inverse_transform(prediction_binarized)
print(predicted_label)

"""# **Testing and evaluating our model**"""

X_test = np.array([
    [168, 65],
    [180, 96],
    [160, 52],
    [169, 67]
])

y_test = ['male', 'male', 'female', 'female']

y_test_binarized = lb.fit_transform(y_test).reshape(-1)

predictions_binarized = model.predict(X_test)

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test_binarized, predictions_binarized)
print(score)

"""# **End of KNN algorithm**

# **Support Vector Machine (SVM)**

# **Training Data**
"""

a = [1, 5, 1.5, 8, 1, 9]
b = [2, 8, 1.8, 8, 0.6, 11]

plt.figure()
plt.scatter(a, b)
plt.show()

X = np.array([
    [1,2],
    [5,8],
    [1.5,1.8],
    [8,8],
    [1,0.6],
    [9,11]
])
y = [0,1,0,1,0,1]

from sklearn import svm

#SVC is support vector classifiers
#C is nothing but how perfectly you want to classify data. Default value is 1.0
#Kernel is a method of using linear classifier to solve non linear problems
model = svm.SVC(kernel='linear', C=1.0)

model.fit(X, y)

prediction = model.predict([[0.58,0.76]])
print(prediction)